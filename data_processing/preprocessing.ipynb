{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk02EmAvZ5Jo"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "FXpqeUX9E1wM"
      },
      "outputs": [],
      "source": [
        "# load library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import io\n",
        "import os\n",
        "import warnings\n",
        "import sys\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import f_classif\n",
        "from scipy.stats import entropy\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkZgjEiqWEIM",
        "outputId": "f98b5e19-92d0-4ccc-c0d5-cfef97f0d059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(55941, 13)\n",
            "(17933, 12)\n",
            "(13598, 13)\n",
            "171\n"
          ]
        }
      ],
      "source": [
        "# load raw data\n",
        "url = \"https://github.com/zxyao5148/STAT3612_2023_1A_GroupProject/raw/main/raw_data/ehr_preprocessed_seq_by_day_cat_embedding.pkl.zip\"\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    with zipfile.ZipFile(io.BytesIO(response.read())) as zip_file:\n",
        "        with zip_file.open(\"ehr_preprocessed_seq_by_day_cat_embedding.pkl\", \"r\") as file:\n",
        "            EHR = pd.read_pickle(file)\n",
        "train = pd.read_csv(\"https://github.com/zxyao5148/STAT3612_2023_1A_GroupProject/raw/main/raw_data/train.csv\")\n",
        "valid = pd.read_csv(\"https://github.com/zxyao5148/STAT3612_2023_1A_GroupProject/raw/main/raw_data/valid.csv\")\n",
        "test = pd.read_csv(\"https://github.com/zxyao5148/STAT3612_2023_1A_GroupProject/raw/main/raw_data/test.csv\")\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(valid.shape)\n",
        "print(len(EHR['feature_cols']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3UTMy_-mXuFR",
        "outputId": "0244d98b-d61f-4127-ca6c-3c9194f2b7f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>hadm_id</th>\n",
              "      <th>admittime</th>\n",
              "      <th>dischtime</th>\n",
              "      <th>deathtime</th>\n",
              "      <th>StudyDate</th>\n",
              "      <th>StudyTime</th>\n",
              "      <th>readmitted_within_30days</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10869829_25238191</td>\n",
              "      <td>10869829</td>\n",
              "      <td>25238191</td>\n",
              "      <td>2141-08-20 12:41:00</td>\n",
              "      <td>2141-09-01 13:22:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21410828</td>\n",
              "      <td>90151.343</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10869829_25238191</td>\n",
              "      <td>10869829</td>\n",
              "      <td>25238191</td>\n",
              "      <td>2141-08-20 12:41:00</td>\n",
              "      <td>2141-09-01 13:22:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21410831</td>\n",
              "      <td>130627.031</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10869829_25238191</td>\n",
              "      <td>10869829</td>\n",
              "      <td>25238191</td>\n",
              "      <td>2141-08-20 12:41:00</td>\n",
              "      <td>2141-09-01 13:22:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21410821</td>\n",
              "      <td>80228.937</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17910612_22301530</td>\n",
              "      <td>17910612</td>\n",
              "      <td>22301530</td>\n",
              "      <td>2188-03-04 19:49:00</td>\n",
              "      <td>2188-04-19 00:00:00</td>\n",
              "      <td>2188-04-19 00:00:00</td>\n",
              "      <td>21880405</td>\n",
              "      <td>200636.062</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17910612_22301530</td>\n",
              "      <td>17910612</td>\n",
              "      <td>22301530</td>\n",
              "      <td>2188-03-04 19:49:00</td>\n",
              "      <td>2188-04-19 00:00:00</td>\n",
              "      <td>2188-04-19 00:00:00</td>\n",
              "      <td>21880321</td>\n",
              "      <td>114135.046</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  subject_id   hadm_id            admittime  \\\n",
              "0  10869829_25238191    10869829  25238191  2141-08-20 12:41:00   \n",
              "1  10869829_25238191    10869829  25238191  2141-08-20 12:41:00   \n",
              "2  10869829_25238191    10869829  25238191  2141-08-20 12:41:00   \n",
              "3  17910612_22301530    17910612  22301530  2188-03-04 19:49:00   \n",
              "4  17910612_22301530    17910612  22301530  2188-03-04 19:49:00   \n",
              "\n",
              "             dischtime            deathtime  StudyDate   StudyTime  \\\n",
              "0  2141-09-01 13:22:00                  NaN   21410828   90151.343   \n",
              "1  2141-09-01 13:22:00                  NaN   21410831  130627.031   \n",
              "2  2141-09-01 13:22:00                  NaN   21410821   80228.937   \n",
              "3  2188-04-19 00:00:00  2188-04-19 00:00:00   21880405  200636.062   \n",
              "4  2188-04-19 00:00:00  2188-04-19 00:00:00   21880321  114135.046   \n",
              "\n",
              "   readmitted_within_30days  \n",
              "0                         0  \n",
              "1                         0  \n",
              "2                         0  \n",
              "3                         1  \n",
              "4                         1  "
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# drop image-related columns\n",
        "train = train.drop(train.columns[[6,7,8,11]], axis=1)\n",
        "valid = valid.drop(valid.columns[[6,7,8,11]], axis=1)\n",
        "test = test.drop(test.columns[[6,7,8,11]], axis=1)\n",
        "train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9271, 9)\n",
            "(2325, 9)\n",
            "(2936, 8)\n"
          ]
        }
      ],
      "source": [
        "# extract last-day observations\n",
        "train['StudyDate'] = pd.to_datetime(train['StudyDate'], format='%Y%m%d')\n",
        "train_latest_date_idxs = train.groupby('id')['StudyDate'].idxmax()\n",
        "train_masked = train.loc[train_latest_date_idxs]\n",
        "\n",
        "valid['StudyDate'] = pd.to_datetime(valid['StudyDate'], format='%Y%m%d')\n",
        "valid_latest_date_idxs = valid.groupby('id')['StudyDate'].idxmax()\n",
        "valid_masked = valid.loc[valid_latest_date_idxs]\n",
        "\n",
        "test['StudyDate'] = pd.to_datetime(test['StudyDate'], format='%Y%m%d')\n",
        "test_latest_date_idxs = test.groupby('id')['StudyDate'].idxmax()\n",
        "test_masked = test.loc[test_latest_date_idxs]\n",
        "\n",
        "print(train_masked.shape)\n",
        "print(valid_masked.shape)\n",
        "print(test_masked.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop rows where deathtime is not empty for train, valid and test\n",
        "#train_masked = train_masked[train_masked['deathtime'].isnull()]\n",
        "#valid_masked = valid_masked[valid_masked['deathtime'].isnull()]\n",
        "#test_masked = test_masked[test_masked['deathtime'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(69539, 9)\n",
            "(11596, 9)\n"
          ]
        }
      ],
      "source": [
        "# merge train and valid\n",
        "train_valid = pd.concat([train, valid])\n",
        "train_valid_masked = pd.concat([train_masked, valid_masked])\n",
        "print(train_valid.shape)\n",
        "print(train_valid_masked.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>hadm_id</th>\n",
              "      <th>admittime</th>\n",
              "      <th>dischtime</th>\n",
              "      <th>deathtime</th>\n",
              "      <th>StudyDate</th>\n",
              "      <th>StudyTime</th>\n",
              "      <th>readmitted_within_30days</th>\n",
              "      <th>stay_len</th>\n",
              "      <th>prev_admits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41856</th>\n",
              "      <td>17195991_23542772</td>\n",
              "      <td>17195991</td>\n",
              "      <td>23542772</td>\n",
              "      <td>2110-01-11 22:47:00</td>\n",
              "      <td>2110-01-18 10:25:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2110-01-16</td>\n",
              "      <td>90654.546</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24332</th>\n",
              "      <td>13721591_20342223</td>\n",
              "      <td>13721591</td>\n",
              "      <td>20342223</td>\n",
              "      <td>2110-02-09 18:13:00</td>\n",
              "      <td>2110-02-22 20:51:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2110-02-19</td>\n",
              "      <td>41948.468</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20119</th>\n",
              "      <td>19170541_22178312</td>\n",
              "      <td>19170541</td>\n",
              "      <td>22178312</td>\n",
              "      <td>2110-02-28 21:48:00</td>\n",
              "      <td>2110-03-12 17:47:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2110-03-11</td>\n",
              "      <td>81842.812</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51567</th>\n",
              "      <td>15554295_27705504</td>\n",
              "      <td>15554295</td>\n",
              "      <td>27705504</td>\n",
              "      <td>2110-03-09 03:54:00</td>\n",
              "      <td>2110-05-18 11:34:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2110-05-04</td>\n",
              "      <td>60653.312</td>\n",
              "      <td>0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29152</th>\n",
              "      <td>17643026_29919541</td>\n",
              "      <td>17643026</td>\n",
              "      <td>29919541</td>\n",
              "      <td>2110-03-25 11:15:00</td>\n",
              "      <td>2110-03-29 17:17:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2110-03-28</td>\n",
              "      <td>140521.453</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id  subject_id   hadm_id           admittime  \\\n",
              "41856  17195991_23542772    17195991  23542772 2110-01-11 22:47:00   \n",
              "24332  13721591_20342223    13721591  20342223 2110-02-09 18:13:00   \n",
              "20119  19170541_22178312    19170541  22178312 2110-02-28 21:48:00   \n",
              "51567  15554295_27705504    15554295  27705504 2110-03-09 03:54:00   \n",
              "29152  17643026_29919541    17643026  29919541 2110-03-25 11:15:00   \n",
              "\n",
              "                dischtime deathtime  StudyDate   StudyTime  \\\n",
              "41856 2110-01-18 10:25:00       NaN 2110-01-16   90654.546   \n",
              "24332 2110-02-22 20:51:00       NaN 2110-02-19   41948.468   \n",
              "20119 2110-03-12 17:47:00       NaN 2110-03-11   81842.812   \n",
              "51567 2110-05-18 11:34:00       NaN 2110-05-04   60653.312   \n",
              "29152 2110-03-29 17:17:00       NaN 2110-03-28  140521.453   \n",
              "\n",
              "       readmitted_within_30days  stay_len  prev_admits  \n",
              "41856                         0         6            0  \n",
              "24332                         0        13            0  \n",
              "20119                         0        11            0  \n",
              "51567                         0        70            0  \n",
              "29152                         0         4            0  "
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add two new predictors (length of stay + previous number of admissions)\n",
        "# length of stay\n",
        "test_masked['dischtime'] = pd.to_datetime(test_masked['dischtime'])\n",
        "test_masked['admittime'] = pd.to_datetime(test_masked['admittime'])\n",
        "test_masked['stay_len'] = (test_masked['dischtime'] - test_masked['admittime']).dt.days\n",
        "valid_masked['dischtime'] = pd.to_datetime(valid_masked['dischtime'])\n",
        "valid_masked['admittime'] = pd.to_datetime(valid_masked['admittime'])\n",
        "valid_masked['stay_len'] = (valid_masked['dischtime'] - valid_masked['admittime']).dt.days\n",
        "train_valid_masked['dischtime'] = pd.to_datetime(train_valid_masked['dischtime'])\n",
        "train_valid_masked['admittime'] = pd.to_datetime(train_valid_masked['admittime'])\n",
        "train_valid_masked['stay_len'] = (train_valid_masked['dischtime'] - train_valid_masked['admittime']).dt.days\n",
        "train_masked['dischtime'] = pd.to_datetime(train_masked['dischtime'])\n",
        "train_masked['admittime'] = pd.to_datetime(train_masked['admittime'])\n",
        "train_masked['stay_len'] = (train_masked['dischtime'] - train_masked['admittime']).dt.days\n",
        "\n",
        "# previous number of admissions\n",
        "test_masked = test_masked.sort_values('admittime')\n",
        "test_masked['prev_admits'] = test_masked.groupby('subject_id').cumcount()\n",
        "valid_masked = valid_masked.sort_values('admittime')\n",
        "valid_masked['prev_admits'] = valid_masked.groupby('subject_id').cumcount()\n",
        "train_valid_masked = train_valid_masked.sort_values('admittime')\n",
        "train_valid_masked['prev_admits'] = train_valid_masked.groupby('subject_id').cumcount()\n",
        "train_masked = train_masked.sort_values('admittime')\n",
        "train_masked['prev_admits'] = train_masked.groupby('subject_id').cumcount()\n",
        "\n",
        "train_masked.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['gender', 'ethnicity', 'Creatinine Blood', 'Sodium Blood', 'pO2 Blood', 'Basophils Other Body Fluid', 'Basophils Pleural', 'Lactate Blood', 'Anion Gap Blood', 'Eosinophils Joint Fluid', 'Hemoglobin Blood', 'Chloride Blood', 'Eosinophils Ascites', 'pH Urine', 'Calcium, Total Blood', 'Lymphocytes Ascites', 'Eosinophils Other Body Fluid', 'Eosinophils Blood', 'Lymphocytes Blood', 'Basophils Joint Fluid', 'Hematocrit Blood', 'Potassium Blood', 'H Blood', 'Monocytes Blood', 'Eosinophils Pleural', 'Troponin T Blood', 'Neutrophils Blood', 'Bicarbonate Blood', 'Basophils Blood', 'Glucose Blood', 'Basophils Ascites', 'pH Blood', 'Platelet Count Blood', 'Lymphocytes Other Body Fluid', 'Monocytes Ascites', 'Lymphocytes Joint Fluid', 'Lymphocytes Pleural', 'pCO2 Blood', 'Y90-Y99', 'G30-G32', 'O85-O92', 'C60-C63', 'F40-F48', 'M80-M85', 'R00-R09', 'J90-J94', 'A00-A09', 'E00-E07', 'F01-F09', 'F30-F39', 'H30-H36', 'D60-D64', 'N00-N08', 'F60-F69', 'I80-I89', 'I95-I99', 'N30-N39', 'K55-K64', 'F50-F59', 'R40-R46', 'J60-J70', 'N20-N23', 'I30-I52', 'R50-R69', 'B25-B34', 'C00-C14', 'D65-D69', 'C73-C75', 'G35-G37', 'E70-E88', 'K20-K31', 'C30-C39', 'I70-I79', 'M60-M63', 'A20-A28', 'N10-N16', 'E89-E89', 'R70-R79', 'C7B-C7B', 'M50-M54', 'A30-A49', 'F20-F29', 'G89-G99', 'R30-R39', 'J30-J39', 'N25-N29', 'Q65-Q79', 'G20-G26', 'B20-B20', 'K65-K68', 'R10-R19', 'E65-E68', 'B65-B83', 'E40-E46', 'F70-F79', 'N17-N19', 'J09-J18', 'J40-J47', 'C15-C26', 'L80-L99', 'B50-B64', 'O60-O77', 'C7A-C7A', 'B85-B89', 'E50-E64', 'K00-K14', 'R20-R23', 'J00-J06', 'N60-N65', 'D37-D48', 'K35-K38', 'G00-G09', 'M05-M14', 'I10-I16', 'B35-B49', 'K40-K46', 'K70-K77', 'Q00-Q07', 'E20-E35', 'J20-J22', 'A80-A89', 'B00-B09', 'J80-J84', 'G60-G65', 'D3A-D3A', 'G10-G14', 'B90-B94', 'N40-N53', 'R90-R94']\n",
            "['ANESTHETICS', 'VITAMINS', 'ANTIBIOTICS', 'ELECT/CALORIC/H2O', 'GASTROINTESTINAL', 'MUSCLE RELAXANTS', 'CARDIOVASCULAR', 'ANTINEOPLASTICS', 'ANTIPARKINSON DRUGS', 'ANTIINFLAM.TUMOR NECROSIS FACTOR INHIBITING AGENTS', 'PRE-NATAL VITAMINS', 'ANTIPARASITICS', 'BLOOD', 'SKIN PREPS', 'CONTRACEPTIVES', 'THYROID PREPS', 'ANTIHISTAMINES', 'EENT PREPS', 'SMOKING DETERRENTS', 'DIAGNOSTIC', 'ANTICOAGULANTS', 'ANTIVIRALS', 'ANTIASTHMATICS', 'PSYCHOTHERAPEUTIC DRUGS', 'CARDIAC DRUGS', 'IMMUNOSUPPRESSANTS', 'ANTIDOTES', 'ANTIARTHRITICS', 'ANTIINFECTIVES/MISCELLANEOUS', 'ANTIPLATELET DRUGS', 'CNS DRUGS', 'ANTIFUNGALS', 'SEDATIVE/HYPNOTICS', 'BIOLOGICALS', 'ANALGESICS', 'AUTONOMIC DRUGS', 'COUGH/COLD PREPARATIONS', 'UNCLASSIFIED DRUG PRODUCTS', 'age', 'ANTIHYPERGLYCEMICS', 'DIURETICS', 'HORMONES']\n"
          ]
        }
      ],
      "source": [
        "# extract feature column names\n",
        "cat_cols = [EHR['feature_cols'][i] for i in EHR['cat_idxs']]\n",
        "icd_cols = EHR['icd_cols']\n",
        "cat_cols.extend(icd_cols)\n",
        "num_cols = list(set(EHR['feature_cols']) - set(cat_cols)) \n",
        "#num_cols.extend(['stay_len', 'prev_admits']) \n",
        "print(cat_cols)\n",
        "print(num_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_valid set\n",
        "train_valid_merged_admits_all = []\n",
        "train_valid_merged_admits_latest = []\n",
        "train_valid_merged_admits_mean_mode = []\n",
        "train_valid_merged_admits_entropy_std = []\n",
        "\n",
        "for _, admit in train_valid_masked.iterrows():\n",
        "    num_feat_arrs = len(EHR['feat_dict'][admit['id']])\n",
        "    temp_dfs = []\n",
        "    for i in range(num_feat_arrs):\n",
        "        feat_arr = pd.Series(EHR['feat_dict'][admit['id']][i], index=EHR['feature_cols'])\n",
        "        merged = admit.append(feat_arr)\n",
        "        merged['obs_id'] = i + 1  \n",
        "        temp_dfs.append(merged.to_frame().transpose())\n",
        "        train_valid_merged_admits_all.append(merged)\n",
        "        if i == num_feat_arrs - 1:\n",
        "            # latest data\n",
        "            feat_arr_latest = pd.Series(EHR['feat_dict'][admit['id']][i], index=EHR['feature_cols'])\n",
        "            train_valid_merged_admits_latest.append(admit.append(feat_arr_latest))\n",
        "            # mean and mode\n",
        "            temp_df = pd.concat(temp_dfs, axis=0)\n",
        "            feat_arr_mean_mode = temp_df[num_cols].mean().append(temp_df[cat_cols].mode().iloc[0])\n",
        "            train_valid_merged_admits_mean_mode.append(admit.append(feat_arr_mean_mode))\n",
        "            # entropy and std\n",
        "            feat_arr_entropy_std = temp_df[num_cols].std().append(temp_df[cat_cols].apply(lambda x: entropy(x.value_counts()), axis=0))\n",
        "            train_valid_merged_admits_entropy_std.append(admit.append(feat_arr_entropy_std))\n",
        "            \n",
        "train_valid_merged_df_all = pd.DataFrame(train_valid_merged_admits_all)\n",
        "train_valid_merged_df_latest = pd.DataFrame(train_valid_merged_admits_latest)\n",
        "train_valid_merged_df_mean_mode = pd.DataFrame(train_valid_merged_admits_mean_mode)\n",
        "train_valid_merged_df_entropy_std = pd.DataFrame(train_valid_merged_admits_entropy_std)\n",
        "\n",
        "# test set\n",
        "test_merged_admits_all = []\n",
        "test_merged_admits_latest = []\n",
        "test_merged_admits_mean_mode = []\n",
        "test_merged_admits_entropy_std = []\n",
        "\n",
        "for _, admit in test_masked.iterrows():\n",
        "    num_feat_arrs = len(EHR['feat_dict'][admit['id']])\n",
        "    temp_dfs = []\n",
        "    for i in range(num_feat_arrs):\n",
        "        feat_arr = pd.Series(EHR['feat_dict'][admit['id']][i], index=EHR['feature_cols'])\n",
        "        merged = admit.append(feat_arr)\n",
        "        merged['obs_id'] = i + 1  \n",
        "        temp_dfs.append(merged.to_frame().transpose())\n",
        "        test_merged_admits_all.append(merged)\n",
        "        if i == num_feat_arrs - 1:\n",
        "            # latest data\n",
        "            feat_arr_latest = pd.Series(EHR['feat_dict'][admit['id']][i], index=EHR['feature_cols'])\n",
        "            test_merged_admits_latest.append(admit.append(feat_arr_latest))\n",
        "            # mean and mode\n",
        "            temp_df = pd.concat(temp_dfs, axis=0)\n",
        "            feat_arr_mean_mode = temp_df[num_cols].mean().append(temp_df[cat_cols].mode().iloc[0])\n",
        "            test_merged_admits_mean_mode.append(admit.append(feat_arr_mean_mode))\n",
        "            # entropy and std\n",
        "            feat_arr_entropy_std = temp_df[num_cols].std().append(temp_df[cat_cols].apply(lambda x: entropy(x.value_counts()), axis=0))\n",
        "            test_merged_admits_entropy_std.append(admit.append(feat_arr_entropy_std))\n",
        "            \n",
        "test_merged_df_all = pd.DataFrame(test_merged_admits_all)\n",
        "test_merged_df_latest = pd.DataFrame(test_merged_admits_latest)\n",
        "test_merged_df_mean_mode = pd.DataFrame(test_merged_admits_mean_mode)\n",
        "test_merged_df_entropy_std = pd.DataFrame(test_merged_admits_entropy_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "# standardization for numeric features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "cat_cols = [EHR['feature_cols'][i] for i in EHR['cat_idxs']]\n",
        "icd_cols = EHR['icd_cols']\n",
        "num_cols = list(set(EHR['feature_cols']) - set(cat_cols) - set(icd_cols)) \n",
        "num_cols.extend(['stay_len', 'prev_admits']) \n",
        "\n",
        "train_valid_merged_df_all[num_cols] = scaler.fit_transform(train_valid_merged_df_all[num_cols])\n",
        "train_valid_merged_df_mean_mode[num_cols] = scaler.fit_transform(train_valid_merged_df_mean_mode[num_cols])\n",
        "train_valid_merged_df_latest[num_cols] = scaler.fit_transform(train_valid_merged_df_latest[num_cols])\n",
        "train_valid_merged_df_entropy_std[num_cols] = scaler.fit_transform(train_valid_merged_df_entropy_std[num_cols])\n",
        "\n",
        "test_merged_df_all[num_cols] = scaler.fit_transform(test_merged_df_all[num_cols])\n",
        "test_merged_df_mean_mode[num_cols] = scaler.fit_transform(test_merged_df_mean_mode[num_cols])\n",
        "test_merged_df_latest[num_cols] = scaler.fit_transform(test_merged_df_latest[num_cols])\n",
        "test_merged_df_entropy_std[num_cols] = scaler.fit_transform(test_merged_df_entropy_std[num_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A20-A28', 'J00-J06']\n",
            "['A20-A28', 'J00-J06', 'Basophils Other Body Fluid', 'Basophils Pleural', 'Eosinophils Joint Fluid', 'pH Urine', 'Basophils Joint Fluid', 'Basophils Ascites', 'Lymphocytes Joint Fluid']\n",
            "['Basophils Other Body Fluid', 'Basophils Pleural', 'Eosinophils Joint Fluid', 'Eosinophils Ascites', 'pH Urine', 'Lymphocytes Ascites', 'Eosinophils Other Body Fluid', 'Basophils Joint Fluid', 'H Blood', 'Basophils Ascites', 'Lymphocytes Other Body Fluid', 'Monocytes Ascites', 'Lymphocytes Joint Fluid', 'A20-A28', 'J00-J06']\n",
            "['MUSCLE RELAXANTS', 'CARDIOVASCULAR', 'ANTINEOPLASTICS', 'ANTIPARKINSON DRUGS', 'ANTIINFLAM.TUMOR NECROSIS FACTOR INHIBITING AGENTS', 'PRE-NATAL VITAMINS', 'ANTIPARASITICS', 'SKIN PREPS', 'CONTRACEPTIVES', 'THYROID PREPS', 'DIAGNOSTIC', 'ANTIVIRALS', 'ANTIDOTES', 'ANTIINFECTIVES/MISCELLANEOUS', 'CNS DRUGS', 'UNCLASSIFIED DRUG PRODUCTS', 'age', 'gender', 'ethnicity', 'Y90-Y99', 'G30-G32', 'O85-O92', 'C60-C63', 'F40-F48', 'M80-M85', 'R00-R09', 'J90-J94', 'A00-A09', 'E00-E07', 'F01-F09', 'F30-F39', 'H30-H36', 'D60-D64', 'N00-N08', 'F60-F69', 'I80-I89', 'I95-I99', 'N30-N39', 'K55-K64', 'F50-F59', 'R40-R46', 'J60-J70', 'N20-N23', 'I30-I52', 'R50-R69', 'B25-B34', 'C00-C14', 'D65-D69', 'C73-C75', 'G35-G37', 'E70-E88', 'K20-K31', 'C30-C39', 'M60-M63', 'A20-A28', 'N10-N16', 'E89-E89', 'R70-R79', 'C7B-C7B', 'M50-M54', 'A30-A49', 'F20-F29', 'G89-G99', 'R30-R39', 'J30-J39', 'N25-N29', 'Q65-Q79', 'G20-G26', 'B20-B20', 'K65-K68', 'R10-R19', 'E65-E68', 'B65-B83', 'E40-E46', 'F70-F79', 'N17-N19', 'J09-J18', 'J40-J47', 'C15-C26', 'L80-L99', 'B50-B64', 'O60-O77', 'C7A-C7A', 'B85-B89', 'E50-E64', 'K00-K14', 'R20-R23', 'J00-J06', 'N60-N65', 'D37-D48', 'K35-K38', 'G00-G09', 'M05-M14', 'B35-B49', 'K40-K46', 'K70-K77', 'Q00-Q07', 'E20-E35', 'J20-J22', 'A80-A89', 'B00-B09', 'G60-G65', 'D3A-D3A', 'G10-G14', 'B90-B94', 'N40-N53', 'R90-R94']\n",
            "(173154, 181)\n",
            "(11596, 173)\n",
            "(11596, 167)\n",
            "(11596, 75)\n"
          ]
        }
      ],
      "source": [
        "# drop features with constant values in the training set\n",
        "train_valid_merged_df_all.insert(2, 'obs_id', train_valid_merged_df_all.pop('obs_id'))\n",
        "test_merged_df_all.insert(2, 'obs_id', test_merged_df_all.pop('obs_id'))\n",
        "\n",
        "feat_cols_all = train_valid_merged_df_all.columns[10:].to_list()\n",
        "feat_cols_latest = train_valid_merged_df_latest.columns[9:].to_list()\n",
        "feat_cols_mean_mode = train_valid_merged_df_mean_mode.columns[9:].to_list()\n",
        "feat_cols_entropy_std = train_valid_merged_df_entropy_std.columns[9:].to_list()\n",
        "\n",
        "const_cols_all = [col for col in feat_cols_all if train_valid_merged_df_all[col].nunique() <= 1]\n",
        "const_cols_latest = [col for col in feat_cols_latest if train_valid_merged_df_latest[col].nunique() <= 1]\n",
        "const_cols_mean_mode = [col for col in feat_cols_mean_mode if train_valid_merged_df_mean_mode[col].nunique() <= 1]\n",
        "const_cols_entropy_std = [col for col in feat_cols_entropy_std if train_valid_merged_df_entropy_std[col].nunique() <= 1]\n",
        "print(const_cols_all)\n",
        "print(const_cols_latest)\n",
        "print(const_cols_mean_mode)\n",
        "print(const_cols_entropy_std)\n",
        "\n",
        "train_valid_merged_df_all = train_valid_merged_df_all.drop(columns=const_cols_all, axis=1)\n",
        "test_merged_df_all = test_merged_df_all.drop(columns=const_cols_all, axis=1)\n",
        "\n",
        "train_valid_merged_df_latest = train_valid_merged_df_latest.drop(columns=const_cols_latest, axis=1)\n",
        "test_merged_df_latest = test_merged_df_latest.drop(columns=const_cols_latest, axis=1)\n",
        "\n",
        "train_valid_merged_df_mean_mode = train_valid_merged_df_mean_mode.drop(columns=const_cols_mean_mode, axis=1)\n",
        "test_merged_df_mean_mode = test_merged_df_mean_mode.drop(columns=const_cols_mean_mode, axis=1)\n",
        "\n",
        "train_valid_merged_df_entropy_std = train_valid_merged_df_entropy_std.drop(columns=const_cols_entropy_std, axis=1)\n",
        "test_merged_df_entropy_std = test_merged_df_entropy_std.drop(columns=const_cols_entropy_std, axis=1)\n",
        "\n",
        "print(train_valid_merged_df_all.shape)\n",
        "print(train_valid_merged_df_latest.shape)\n",
        "print(train_valid_merged_df_mean_mode.shape)\n",
        "print(train_valid_merged_df_entropy_std.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make copies\n",
        "#train_valid_merged_df_all_copy = train_valid_merged_df_all.copy()\n",
        "#train_valid_merged_df_median_copy = train_valid_merged_df_median.copy()\n",
        "#train_valid_merged_df_latest_copy = train_valid_merged_df_latest.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop info columns\n",
        "#train_valid_merged_df_all = train_valid_merged_df_all.drop(train_valid_merged_df_all.columns[0:9], axis=1)\n",
        "#train_valid_merged_df_latest = train_valid_merged_df_latest.drop(train_valid_merged_df_latest.columns[0:8], axis=1)\n",
        "#train_valid_merged_df_median = train_valid_merged_df_median.drop(train_valid_merged_df_median.columns[0:8], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [],
      "source": [
        "# feature selection with correlation test\n",
        "# 1. drop features with correlation (with response) lower than 0.1 (5/10 fold correlation test)\n",
        "# 2. if multiple features have correlation > 0.7 between each other, only keep the one with the highest correlation with response\n",
        "\n",
        "#train_merged_df_all\n",
        "#STEP1\n",
        "# feature selection\n",
        "# 1. drop features with correlation (with response) lower than 0.1 (5/10 fold correlation test)\n",
        "# 2. if multiple features have correlation > 0.7 between each other, only keep the one with the highest correlation with response\n",
        "\n",
        "#train_merged_df_all\n",
        "#STEP1\n",
        "\n",
        "#k = 10\n",
        "#cv = KFold(n_splits=k, shuffle=True)\n",
        "#selected_features = []\n",
        "#for train_index, _ in cv.split(train_merged_df_all):\n",
        "#    fold_data = train_merged_df_all.iloc[train_index]\n",
        "#    numeric_data = fold_data.select_dtypes(include=np.number) \n",
        "#    correlations = numeric_data.corrwith(fold_data['readmitted_within_30days'])\n",
        "#    selected_features.append(correlations)\n",
        "#compute average correlation\n",
        "#selected_features_df = pd.concat(selected_features, axis=1)\n",
        "#average_correlation = selected_features_df.mean(axis=1)\n",
        "#drop variables with correlation with response lower than 0.01\n",
        "#selected_columns = average_correlation[average_correlation < 0.001].index\n",
        "#train_selected_df_all = train_merged_df_all.drop(selected_columns, axis=1)\n",
        "#STEP2\n",
        "#corr_matrix = train_selected_df_all.corr()\n",
        "#cols = corr_matrix.columns\n",
        "#drop variables with correlation with correlation between each other>0.7 except the highest one\n",
        "#for i in range(len(cols)):\n",
        "#    for j in range(i+1, len(cols)):\n",
        "#        if abs(corr_matrix.iloc[i,j]) > 0.7:\n",
        "#            if abs(corr_matrix.iloc[i,-1]) > abs(corr_matrix.iloc[j,-1]):\n",
        "#                train_selected_df_all = train_selected_df_all.drop(cols[j], axis=1)\n",
        "#            else:\n",
        "#                train_selected_df_all = train_selected_df_all.drop(cols[i], axis=1)\n",
        "#                break\n",
        "#print(train_selected_df_all.shape)\n",
        "\n",
        "#train_merged_df_latest\n",
        "#STEP1\n",
        "#k = 10\n",
        "#cv = KFold(n_splits=k, shuffle=True)\n",
        "#selected_features = []\n",
        "#for train_index, _ in cv.split(train_merged_df_latest):\n",
        "#    fold_data = train_merged_df_latest.iloc[train_index]\n",
        "#    numeric_data = fold_data.select_dtypes(include=np.number) \n",
        "#    correlations = numeric_data.corrwith(fold_data['readmitted_within_30days'])\n",
        "#    selected_features.append(correlations)\n",
        "#selected_features_df = pd.concat(selected_features, axis=1)\n",
        "#average_correlation = selected_features_df.mean(axis=1)\n",
        "#selected_columns = average_correlation[average_correlation < 0.001].index\n",
        "#train_selected_df_latest = train_merged_df_latest.drop(selected_columns, axis=1)\n",
        "#STEP2\n",
        "#corr_matrix = train_selected_df_latest.corr()\n",
        "#cols = corr_matrix.columns\n",
        "#for i in range(len(cols)):\n",
        "#    for j in range(i+1, len(cols)):\n",
        "#        if abs(corr_matrix.iloc[i,j]) > 0.7:\n",
        "#            if abs(corr_matrix.iloc[i,-1]) > abs(corr_matrix.iloc[j,-1]):\n",
        "#                train_selected_df_latest = train_selected_df_latest.drop(cols[j], axis=1)\n",
        "#            else:\n",
        "#                train_selected_df_latest = train_selected_df_latest.drop(cols[i], axis=1)\n",
        "#                break\n",
        "#print(train_selected_df_latest.shape)\n",
        "\n",
        "#train_merged_df_median\n",
        "#STEP1\n",
        "#k = 10\n",
        "#cv = KFold(n_splits=k, shuffle=True)\n",
        "#selected_features = []\n",
        "#for train_index, _ in cv.split(train_merged_df_median):\n",
        "#    fold_data = train_merged_df_median.iloc[train_index]\n",
        "#    numeric_data = fold_data.select_dtypes(include=np.number) \n",
        "#    correlations = numeric_data.corrwith(fold_data['readmitted_within_30days'])\n",
        "#    selected_features.append(correlations)\n",
        "#selected_features_df = pd.concat(selected_features, axis=1)\n",
        "#average_correlation = selected_features_df.mean(axis=1)\n",
        "#selected_columns = average_correlation[average_correlation < 0.001].index\n",
        "#train_selected_df_median = train_merged_df_median.drop(selected_columns, axis=1)\n",
        "#STEP2\n",
        "#corr_matrix = train_selected_df_median.corr()\n",
        "#cols = corr_matrix.columns\n",
        "#for i in range(len(cols)):\n",
        "#    for j in range(i+1, len(cols)):\n",
        "#        if abs(corr_matrix.iloc[i,j]) > 0.7:\n",
        "#            if abs(corr_matrix.iloc[i,-1]) > abs(corr_matrix.iloc[j,-1]):\n",
        "#                train_selected_df_median = train_selected_df_median.drop(cols[j], axis=1)\n",
        "#            else:\n",
        "#                train_selected_df_median = train_selected_df_median.drop(cols[i], axis=1)\n",
        "#                break\n",
        "#print(train_selected_df_median.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train_valid_merged_df_all\n",
        "#STEP1\n",
        "#k = 10\n",
        "#cv = KFold(n_splits=k, shuffle=True)\n",
        "#selected_features = []\n",
        "#for train_valid_index, _ in cv.split(train_valid_merged_df_all):\n",
        "#    fold_data = train_valid_merged_df_all.iloc[train_valid_index]\n",
        "#    numeric_data = fold_data.select_dtypes(include=np.number) \n",
        "#    correlations = numeric_data.corrwith(fold_data['readmitted_within_30days'])\n",
        "#    selected_features.append(correlations)\n",
        "#selected_features_df = pd.concat(selected_features, axis=1)\n",
        "#average_correlation = selected_features_df.mean(axis=1)\n",
        "#selected_columns = average_correlation[average_correlation < 0.001].index\n",
        "#train_valid_selected_df_all = train_valid_merged_df_all.drop(selected_columns, axis=1)\n",
        "#STEP2\n",
        "#corr_matrix = train_valid_selected_df_all.corr()\n",
        "#cols = corr_matrix.columns\n",
        "#for i in range(len(cols)):\n",
        "#    for j in range(i+1, len(cols)):\n",
        "#        if abs(corr_matrix.iloc[i,j]) > 0.7:\n",
        "#            if abs(corr_matrix.iloc[i,-1]) > abs(corr_matrix.iloc[j,-1]):\n",
        "#                train_valid_selected_df_all = train_valid_selected_df_all.drop(cols[j], axis=1)\n",
        "#            else:\n",
        "#                train_valid_selected_df_all = train_valid_selected_df_all.drop(cols[i], axis=1)\n",
        "#                break\n",
        "#print(train_valid_selected_df_all.shape)\n",
        "\n",
        "#train_valid_merged_df_latest\n",
        "#STEP1\n",
        "#k = 10\n",
        "#cv = KFold(n_splits=k)\n",
        "#selected_features = []\n",
        "#for train_valid_index, _ in cv.split(train_valid_merged_df_latest):\n",
        "#    fold_data = train_valid_merged_df_latest.iloc[train_valid_index]\n",
        "#    numeric_data = fold_data.select_dtypes(include=np.number) \n",
        "#    correlations = numeric_data.corrwith(fold_data['readmitted_within_30days'])\n",
        "#    selected_features.append(correlations)\n",
        "#selected_features_df = pd.concat(selected_features, axis=1)\n",
        "#average_correlation = selected_features_df.mean(axis=1)\n",
        "#selected_columns = average_correlation[average_correlation < 0.001].index\n",
        "#train_valid_selected_df_latest = train_valid_merged_df_latest.drop(selected_columns, axis=1)\n",
        "#STEP2\n",
        "#corr_matrix = train_valid_selected_df_latest.corr()\n",
        "#cols = corr_matrix.columns\n",
        "#for i in range(len(cols)):\n",
        "#    for j in range(i+1, len(cols)):\n",
        "#        if abs(corr_matrix.iloc[i,j]) > 0.7:\n",
        "#            if abs(corr_matrix.iloc[i,-1]) > abs(corr_matrix.iloc[j,-1]):\n",
        "#                train_valid_selected_df_latest = train_valid_selected_df_latest.drop(cols[j], axis=1)\n",
        "#            else:\n",
        "#                train_valid_selected_df_latest = train_valid_selected_df_latest.drop(cols[i], axis=1)\n",
        "#                break\n",
        "#print(train_valid_selected_df_latest.shape)\n",
        "\n",
        "#train_valid_merged_df_median\n",
        "#STEP1\n",
        "#k = 10\n",
        "#cv = KFold(n_splits=k)\n",
        "#selected_features = []\n",
        "#for train_valid_index, _ in cv.split(train_valid_merged_df_median):\n",
        "#    fold_data = train_valid_merged_df_median.iloc[train_valid_index]\n",
        "#    numeric_data = fold_data.select_dtypes(include=np.number) \n",
        "#    correlations = numeric_data.corrwith(fold_data['readmitted_within_30days'])\n",
        "#    selected_features.append(correlations)\n",
        "#selected_features_df = pd.concat(selected_features, axis=1)\n",
        "#average_correlation = selected_features_df.mean(axis=1)\n",
        "#selected_columns = average_correlation[average_correlation < 0.001].index\n",
        "#train_valid_selected_df_median = train_valid_merged_df_median.drop(selected_columns, axis=1)\n",
        "#STEP2\n",
        "#corr_matrix = train_valid_selected_df_median.corr()\n",
        "#cols = corr_matrix.columns\n",
        "#for i in range(len(cols)):\n",
        "#    for j in range(i+1, len(cols)):\n",
        "#        if abs(corr_matrix.iloc[i,j]) > 0.7:\n",
        "#            if abs(corr_matrix.iloc[i,-1]) > abs(corr_matrix.iloc[j,-1]):\n",
        "#                train_valid_selected_df_median = train_valid_selected_df_median.drop(cols[j], axis=1)\n",
        "#            else:\n",
        "#                train_valid_selected_df_median = train_valid_selected_df_median.drop(cols[i], axis=1)\n",
        "#                break\n",
        "#print(train_valid_selected_df_median.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add suffix\n",
        "train_valid_merged_df_entropy_std.columns = list(train_valid_merged_df_entropy_std.columns[:9]) + [str(col) + '_entropy_std' for col in train_valid_merged_df_entropy_std.columns[9:]]\n",
        "test_merged_df_entropy_std.columns = list(test_merged_df_entropy_std.columns[:8]) + [str(col) + '_entropy_std' for col in test_merged_df_entropy_std.columns[8:]]\n",
        "\n",
        "train_valid_merged_df_mean_mode.columns = list(train_valid_merged_df_mean_mode.columns[:9]) + [str(col) + '_mean_mode' for col in train_valid_merged_df_mean_mode.columns[9:]]\n",
        "test_merged_df_mean_mode.columns = list(test_merged_df_mean_mode.columns[:8]) + [str(col) + '_mean_mode' for col in test_merged_df_mean_mode.columns[8:]]\n",
        "\n",
        "train_valid_merged_df_latest.columns = list(train_valid_merged_df_latest.columns[:9]) + [str(col) + '_latest' for col in train_valid_merged_df_latest.columns[9:]]\n",
        "test_merged_df_latest.columns = list(test_merged_df_latest.columns[:8]) + [str(col) + '_latest' for col in test_merged_df_latest.columns[8:]]\n",
        "\n",
        "train_valid_merged_df_all.columns = list(train_valid_merged_df_all.columns[:9]) + [str(col) + '_all' for col in train_valid_merged_df_all.columns[9:]]\n",
        "test_merged_df_all.columns = list(test_merged_df_all.columns[:8]) + [str(col) + '_all' for col in test_merged_df_all.columns[8:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write csv files\n",
        "# train set\n",
        "train_valid_merged_df_latest.to_csv('train/train_valid_latest.csv', index=False)\n",
        "train_valid_merged_df_mean_mode.to_csv('train/train_valid_mean_mode.csv', index=False)\n",
        "train_valid_merged_df_all.to_csv('train/train_valid_all.csv', index=False)\n",
        "train_valid_merged_df_entropy_std.to_csv('train/train_valid_entropy_std.csv', index=False)\n",
        "\n",
        "with zipfile.ZipFile('train/sequence_data.zip', 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipf.write('train/train_valid_all.csv')\n",
        "os.remove('train/train_valid_all.csv')\n",
        "\n",
        "# test set\n",
        "test_merged_df_latest.to_csv('test/test_latest.csv', index=False)\n",
        "test_merged_df_mean_mode.to_csv('test/test_mean_mode.csv', index=False)\n",
        "test_merged_df_entropy_std.to_csv('test/test_entropy_std.csv', index=False)\n",
        "test_merged_df_all.to_csv('test/test_all.csv', index=False)\n",
        "with zipfile.ZipFile('test/sequence_data.zip', 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipf.write('test/test_all.csv')\n",
        "os.remove('test/test_all.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
